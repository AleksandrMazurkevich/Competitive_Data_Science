{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" />\n",
    "\n",
    "# Что такое W&B?\n",
    "\n",
    "`Weights & Biases (W&B)` - это многофункциональная платформа для отслеживания экспериментов, логирования результатов, которая позволяет облегчить и ускорить рутинные процессы при обучении моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ключевые особенности\n",
    "\n",
    "* Позволяет отслеживать показатели работы модели в режиме реального времени и сразу же выявляйте проблемные места.\n",
    "* Визуализация результатов - графики, изображения, видео, аудио, 3D-объекты и многое [другое](https://docs.wandb.ai/guides/track/log#logging-objects).\n",
    "* Позволяет тренировать модели с разных устройств и хранить результаты в одном месте, так же удобно при командной работе над задачей\n",
    "* Использование артефактов W&B позволяет отслеживать и создавать версии ваших наборов данных, моделей, зависимостей и результатов в конвейерах машинного обучения.\n",
    "* Низкий порог входа (можно начать с 6 строк кода), так же есть готовые интеграции со всеми популярными DS фреймворками\n",
    "\n",
    "![centralized dashboard](https://i.imgur.com/BGgfZj3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Установка `wandb`\n",
    "\n",
    "`wandb` (библиотека W&B), по умолчанию установлена на Kagglе\n",
    "\n",
    "Поскольку образ kaggle не часто обновляется, а `wandb` постоянно выпускает новые версии,рекомендуется установить последнюю версию с помощью флага `--upgrade`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade -q wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регистрация учетной записи и вход в систему\n",
    "\n",
    "Вам понадобится уникальный ключ API для входа в Weights & Biases.\n",
    "\n",
    "1. Если у вас нет учетной записи Weights & Biases, вы можете перейти на https://wandb.ai/site и создать БЕСПЛАТНУЮ учетную запись.\n",
    "2. Доступ к ключу API: https://wandb.ai/authorize.\n",
    "\n",
    "На Kaggle можно авторизоваться в W&B двумя способами:\n",
    "\n",
    "1. С помощью `wandb.login ()`. Он запросит ключ API, который вы можете скопировать + вставить.\n",
    "2. Используя Kaggle secrets для хранения ключа API и использовать приведенный ниже фрагмент кода для входа в систему. Прочтите это [обсуждение](https://www.kaggle.com/product-feedback/114053), чтобы узнать больше о Kaggle secrets.\n",
    "\n",
    "```python\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "wandb_api = user_secrets.get_secret(\"wandb_api\") \n",
    "wandb.login(key=wandb_api)\n",
    "```\n",
    "[Подробнее о входе в W&B](https://docs.wandb.ai/ref/cli/wandb-login)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mivanich\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Возьмем, для примера, реализацию `MLP` из раздела про нейросети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import copy\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Загружаем файлы датасета\n",
    "\n",
    "car_info = pd.read_csv('../data/car_info.csv')   # car_info - информация про машины с таргетом\n",
    " \n",
    "rides_info = pd.read_csv('../data/rides_info.csv') # rides_info - информация про поездки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rides_info = rides_info.merge(car_info, on = 'car_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "drop_cols = ['user_id', 'car_id', 'ride_id', 'ride_date']\n",
    "cat_cols = ['car_type', 'fuel_type', 'model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_117170/3567080960.py:5: FutureWarning: The default value of numeric_only in DataFrame.median is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  rides_info.fillna(rides_info.median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# закодируем категориальные фичи в one hot encoding вектора\n",
    "rides_info = pd.get_dummies(rides_info, columns=cat_cols)\n",
    "\n",
    "# заполним пропущенные значения медианным значением по столбцу\n",
    "rides_info.fillna(rides_info.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rides_df = rides_info.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# переведем строковые значения категориального таргета в целочисленные\n",
    "le = LabelEncoder()\n",
    "rides_df['target_class'] = le.fit_transform(rides_df['target_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# числовые переменные, которые подвергнем трансформации\n",
    "num_cols = [col for col in list(rides_df.columns)\n",
    "            if col not in ['target_reg', 'target_class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "df = scaler.fit_transform(rides_info[num_cols])\n",
    "df = pd.DataFrame(df, columns=num_cols)\n",
    "\n",
    "target_scaler = RobustScaler()\n",
    "target = target_scaler.fit_transform(rides_info['target_reg'].values.reshape(-1, 1))\n",
    "\n",
    "df['target_reg'] = target\n",
    "df['target_class'] = rides_info['target_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ВАЖНО! - фиксируем воспроизводимость\n",
    "def seed_everything(seed=42):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# для наших данных и размера нейросети подойдет запуск на cpu\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохраняем гиперпараметры в виде словаря, для того чтобы позже зарегистрировать этот конфигурационный файл в W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Определяем гиперпараметры\n",
    "\n",
    "CONFIG = dict (\n",
    "    hidden_size=128,\n",
    "    dropout=0.1,\n",
    "    lr=1e-3,\n",
    "    batch_size=128,\n",
    "    num_workers=os.cpu_count(),\n",
    "    epochs=10,\n",
    "    num_features=train.shape[1]-2, # кол-во фичей подаваемое на вход\n",
    "    num_tar_2=train.target_class.nunique(), # количество выходов равно кол-ву предсказываемых классов\n",
    "    architecture = \"MLP\",\n",
    "    infra = \"Kaggle\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Строим входной конвейер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# датасет выдает фичи и значения целевых переменных\n",
    "class Rides(Dataset):\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx,:]\n",
    "        \n",
    "        data = row.drop(labels=['target_reg', 'target_class'])\n",
    "        \n",
    "        data = torch.FloatTensor(data.values.astype('float'))\n",
    "        tar_1 = torch.tensor(row['target_reg']).float()\n",
    "        tar_2 = row['target_class'].astype('int')\n",
    "        \n",
    "        return data, tar_1, tar_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build data loaders\n",
    "\n",
    "train_datasets = {'train': Rides(train),\n",
    "                  'val': Rides(test)}\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(train_datasets[x], \n",
    "                                                   batch_size=CONFIG['batch_size'], \n",
    "                                                   shuffle=True, \n",
    "                                                   num_workers=CONFIG['num_workers'])\n",
    "                    for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Настраиваем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Построим архитектуру mlp с двумя головами для регрессии и классифкации\n",
    "\n",
    "class TabularNN(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "                          nn.Linear(cfg['num_features'], cfg['hidden_size']),\n",
    "                          nn.Dropout(cfg['dropout']),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(cfg['hidden_size'], cfg['hidden_size']),\n",
    "                          nn.Dropout(cfg['dropout']),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(cfg['hidden_size'], cfg['hidden_size'] // 2),\n",
    "                          )\n",
    "        \n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(cfg['hidden_size'] // 2, 1)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(cfg['hidden_size'] // 2, cfg['num_tar_2'])\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.mlp(data)\n",
    "        tar1 = self.regressor(x)\n",
    "        tar2 = self.classifier(x)\n",
    "        return tar1.view(-1), tar2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучаем и логируем результаты в W&B\n",
    "\n",
    "В этом разделе будем использовать:\n",
    "\n",
    "* [`wandb.init()`](https://docs.wandb.ai/guides/track/launch): инициализировать новый запуск.\n",
    "* [`wandb.finish()`](https://docs.wandb.ai/ref/python/finish):  завершить и закрыть пробег.\n",
    "* [`wandb.config`](https://docs.wandb.ai/guides/track/config): объект, в котором хранятся гиперпараметры и настройки, связанные с запуском.\n",
    "\n",
    "\n",
    "Run (или [объект wandb.Run](https://docs.wandb.ai/ref/python/run)) - это единица вычисления W&B, обычно это 1 эксперимент обучения модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = TabularNN(CONFIG).to(device)\n",
    "\n",
    "# оптимайзер и лоссы для регрессии и классификации\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = CONFIG['lr'])\n",
    "regression_criterion = nn.MSELoss().to(device)\n",
    "classification_criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Используем `wandb.init ()` для инициализации нового прогона W&B.\n",
    "\n",
    "В \"pipeline\" машинного обучения вы можете добавить `wandb.init ()` в начало обучающего сценария, а также сценарий оценки, и каждая часть будет отслеживаться как запуск в W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:  {'hidden_size': 128, 'dropout': 0.1, 'lr': 0.001, 'batch_size': 128, 'num_workers': 32, 'epochs': 10, 'num_features': 46, 'num_tar_2': 9, 'architecture': 'MLP', 'infra': 'Kaggle', 'model_name': 'Tabular_NN'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/storage/Aleron/Course/Competitive_Data_Science/notebooks/wandb/run-20230424_130947-7npv8nzq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ivanich/Course%20contest/runs/7npv8nzq' target=\"_blank\">smooth-leaf-23</a></strong> to <a href='https://wandb.ai/ivanich/Course%20contest' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ivanich/Course%20contest' target=\"_blank\">https://wandb.ai/ivanich/Course%20contest</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ivanich/Course%20contest/runs/7npv8nzq' target=\"_blank\">https://wandb.ai/ivanich/Course%20contest/runs/7npv8nzq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Добавим в CONFIG имя модели\n",
    "CONFIG['model_name'] = 'Tabular_NN'\n",
    "print('Training configuration: ', CONFIG)\n",
    "\n",
    "# Initialize W&B run\n",
    "run = wandb.init(project='Course contest',\n",
    "                 entity=\"ivanich\",\n",
    "                 config=CONFIG,\n",
    "                 group='MLP', \n",
    "                 job_type='train')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ипользуемые аргументы `wandb.init ()`:\n",
    "\n",
    "* `project`: этот аргумент указывает имя проекта W&B, в который отправляется запуск. Создаем на платформе W&B новый проект с названием «CDSC_DataFeeling_contest» и одновременно отправляем в него run.\n",
    "\n",
    "* `config`: этот аргумент устанавливает `wandb.config`, объект, подобный словарю, в котором хранятся гиперпараметры, параметры ввода и другие независимые переменные.\n",
    "\n",
    "* `group`: этот аргумент указывает значение для группировки отдельных прогонов, чтобы было легче сравнивать прогоны из разных архитектур.\n",
    "\n",
    "* `job_type`: этот аргумент указывает тип выполнения, например, «`train`» или «`evaluate`». Установка типа run упрощает последующую фильтрацию и группировку run, например, чтобы сравнить несколько «`train`» run-ов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обновляем `wandb.config` \n",
    "\n",
    "Сохранение конфигурации тренировки полезно для анализа экспериментов и воспроизведения вашей работы позже. С помощью W&B вы также можете группировать run-ы по значениям конфигурации, что означает, что вы можете сравнивать настройки разных run-ов и видеть, как они влияют на результат.\n",
    "\n",
    "Есть несколько способов настроить `wandb.config`:\n",
    "\n",
    "* Задать `wandb.config` с аргументом `wandb.init (config)`, как указано выше.\n",
    "* Установить `wandb.config` напрямую.\n",
    "* См. Дополнительные параметры настройки в этом [Colab](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-log/Configs_in_W%26B.ipynb#scrollTo=xFf3zjBSixC1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">smooth-leaf-23</strong> at: <a href='https://wandb.ai/ivanich/Course%20contest/runs/7npv8nzq' target=\"_blank\">https://wandb.ai/ivanich/Course%20contest/runs/7npv8nzq</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230424_130947-7npv8nzq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add \"type\" and \"kaggle_competition\" to `wandb.config` directly\n",
    "wandb.config.type = 'baseline'\n",
    "wandb.config.kaggle_competition = 'Competitive Data Science Course by Data Feeling'\n",
    "\n",
    "# Close W&B run\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проведем серию из 5 экспериментов для подбора параметра `dropout rate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Изменим CONFIG - определим dropout как рандомную величину в заданном диапазоне значений\n",
    "import random\n",
    "\n",
    "CONFIG['dropout'] = random.uniform(0.01, 0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385abf88281a44848534563488cafdc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666812083373467, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/storage/Aleron/Course/Competitive_Data_Science/notebooks/wandb/run-20230424_130959-1ljkabm6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ivanich/Course%20contest/runs/1ljkabm6' target=\"_blank\">northern-fog-24</a></strong> to <a href='https://wandb.ai/ivanich/Course%20contest' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ivanich/Course%20contest' target=\"_blank\">https://wandb.ai/ivanich/Course%20contest</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ivanich/Course%20contest/runs/1ljkabm6' target=\"_blank\">https://wandb.ai/ivanich/Course%20contest/runs/1ljkabm6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_117170/4292527571.py\", line 17, in __getitem__\n    tar_2 = row['target_class'].astype('int')\nAttributeError: 'str' object has no attribute 'astype'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Iterate over data.\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels_1, labels_2 \u001b[38;5;129;01min\u001b[39;00m dataloaders[phase]:\n\u001b[1;32m     31\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     32\u001b[0m     labels_1 \u001b[38;5;241m=\u001b[39m labels_1\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1333\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1359\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/_utils.py:543\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 543\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mAttributeError\u001b[0m: Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_117170/4292527571.py\", line 17, in __getitem__\n    tar_2 = row['target_class'].astype('int')\nAttributeError: 'str' object has no attribute 'astype'\n"
     ]
    }
   ],
   "source": [
    "num_epochs = CONFIG['epochs']\n",
    "for _ in range(5):\n",
    "    # 🐝 initialise a wandb run\n",
    "    wandb.init(project='Course contest',\n",
    "                 entity=\"ivanich\", \n",
    "                config=CONFIG,\n",
    "                group='MLP', \n",
    "                job_type='train'\n",
    "            )\n",
    "    \n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 100000.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels_1, labels_2 in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels_1 = labels_1.to(device)\n",
    "                labels_2 = labels_2.to(device)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "\n",
    "                    outputs_1, outputs_2 = model(inputs)\n",
    "                    loss_1 = regression_criterion(outputs_1, labels_1)\n",
    "                    loss_2 = classification_criterion(outputs_2, labels_2)\n",
    "\n",
    "                    loss = loss_1 + loss_2\n",
    "\n",
    "                    _, preds_2 = torch.max(outputs_2, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                val_acc_history.append(running_loss)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "            \n",
    "            # 🐝 Log train and validation metrics to wandb\n",
    "            wandb.log({'{} loss'.format(phase): epoch_loss})\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    # 🐝 Close your wandb run \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Используйте `run.finish ()`, чтобы закрыть инициализированный run W&B после завершения `job_type`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Каждый run получает свою собственную страницу, на вкладках которой содержится дополнительная информация о запуске.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание артефакта W&B.\n",
    "\n",
    "W&B Artifacts позволяет вам регистрировать данные, которые входят (например, набор данных) и выходят (например, веса обученной модели) этих процессов.\n",
    "\n",
    "Другими словами, артефакты - это способ сохранить ваши наборы данных и модели. Вы можете использовать [этот Colab](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-artifacts/Pipeline_Versioning_with_W%26B_Artifacts.ipynb), чтобы узнать больше об артефактах.\n",
    "\n",
    "### Сохраняем свою работу с помощью `wandb.log_artifact ()` \n",
    "\n",
    "В run есть три шага для создания и сохранения артефакта модели.\n",
    "\n",
    "1. Создайте пустой артефакт с помощью `wandb.Artifact ()`.\n",
    "2. Добавьте файл модели в Артефакт с помощью `wandb.add_file ()`.\n",
    "3. Вызовите `wandb.log_artifact ()`, чтобы сохранить Артефакт."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), 'tab_model.pth')\n",
    "\n",
    "# Initialize a new W&B run\n",
    "run = wandb.init(project='Course contest',\n",
    "                 entity=\"ivanich\", \n",
    "                config=CONFIG,\n",
    "                group='MLP', \n",
    "                 job_type='save') # Note the job_type\n",
    "\n",
    "# Update `wandb.config`\n",
    "wandb.config.type = 'baseline'\n",
    "wandb.config.kaggle_competition = 'Competitive Data Science Course by Data Feeling'\n",
    "\n",
    "# Save model as Model Artifact\n",
    "artifact = wandb.Artifact(name='best_tab_NN', type='model') # Задаем произвольное имя\n",
    "artifact.add_file('tab_model.pth')\n",
    "run.log_artifact(artifact)\n",
    "\n",
    "# Finish W&B run\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Теперь залогируем бустинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import json\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1ljkabm6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">northern-fog-24</strong> at: <a href='https://wandb.ai/ivanich/Course%20contest/runs/1ljkabm6' target=\"_blank\">https://wandb.ai/ivanich/Course%20contest/runs/1ljkabm6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230424_130959-1ljkabm6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1ljkabm6). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb01705488b3415f889e15623db9fd1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668017068877817, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/storage/Aleron/Course/Competitive_Data_Science/notebooks/wandb/run-20230424_131348-vv619c6y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ivanich/Course%20contest/runs/vv619c6y' target=\"_blank\">fearless-fog-25</a></strong> to <a href='https://wandb.ai/ivanich/Course%20contest' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ivanich/Course%20contest' target=\"_blank\">https://wandb.ai/ivanich/Course%20contest</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ivanich/Course%20contest/runs/vv619c6y' target=\"_blank\">https://wandb.ai/ivanich/Course%20contest/runs/vv619c6y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project='Course contest',\n",
    "                 entity=\"ivanich\",\n",
    "                 job_type='train-model',\n",
    "                 group='XGB',\n",
    "                 config={'wandb_nb':'wandb_course_contest'})  # config is optional here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_params = {\n",
    "         'gamma': 1,               ## def: 0\n",
    "         'learning_rate': 0.1,     ## def: 0.1\n",
    "        'max_depth': 3,\n",
    "        'min_child_weight': 100,  ## def: 1\n",
    "        'n_estimators': 25,\n",
    "        'nthread': 4,\n",
    "        'random_state': 42,\n",
    "        'reg_alpha': 0,\n",
    "        'reg_lambda': 0,          ## def: 1\n",
    "        'eval_metric': [ 'mlogloss'],\n",
    "        'tree_method': 'hist'  # use `gpu_hist` to train on GPU\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.config.update(dict(bst_params))\n",
    "run.config.update({'early_stopping_rounds': 40})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['target_reg','target_class'], axis=1)\n",
    "y = rides_df['target_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В `wandb` есть встроенная поддержка библиотеки XGBoost через callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's multi_logloss: 2.12266\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[2]\tvalid_0's multi_logloss: 2.07981\n",
      "[3]\tvalid_0's multi_logloss: 2.0463\n",
      "[4]\tvalid_0's multi_logloss: 2.01899\n",
      "[5]\tvalid_0's multi_logloss: 1.99613\n",
      "[6]\tvalid_0's multi_logloss: 1.97649\n",
      "[7]\tvalid_0's multi_logloss: 1.95974\n",
      "[8]\tvalid_0's multi_logloss: 1.94527\n",
      "[9]\tvalid_0's multi_logloss: 1.9326\n",
      "[10]\tvalid_0's multi_logloss: 1.92094\n",
      "[11]\tvalid_0's multi_logloss: 1.91083\n",
      "[12]\tvalid_0's multi_logloss: 1.9013\n",
      "[13]\tvalid_0's multi_logloss: 1.8926\n",
      "[14]\tvalid_0's multi_logloss: 1.88489\n",
      "[15]\tvalid_0's multi_logloss: 1.87779\n",
      "[16]\tvalid_0's multi_logloss: 1.87106\n",
      "[17]\tvalid_0's multi_logloss: 1.86467\n",
      "[18]\tvalid_0's multi_logloss: 1.85874\n",
      "[19]\tvalid_0's multi_logloss: 1.84907\n",
      "[20]\tvalid_0's multi_logloss: 1.84341\n",
      "[21]\tvalid_0's multi_logloss: 1.83864\n",
      "[22]\tvalid_0's multi_logloss: 1.83376\n",
      "[23]\tvalid_0's multi_logloss: 1.82731\n",
      "[24]\tvalid_0's multi_logloss: 1.82192\n",
      "[25]\tvalid_0's multi_logloss: 1.81685\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[25]\tvalid_0's multi_logloss: 1.81685\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x7f4c6cccceb0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wandb.lightgbm import wandb_callback, log_summary\n",
    "import lightgbm as lgb\n",
    "\n",
    "xgbmodel = lgb.LGBMClassifier(**bst_params, use_label_encoder=False)\n",
    "\n",
    "# Train the model, using the wandb_callback for logging\n",
    "xgbmodel.fit(X_train, y_train, eval_set=[(X_test, y_test)], \n",
    "             early_stopping_rounds=run.config['early_stopping_rounds'],\n",
    "             callbacks=[wandb_callback()])\n",
    "\n",
    "bstr = xgbmodel.booster_\n",
    "bstr.save_model(\"lgb.mod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wandb.xgboost import wandb_callback\n",
    "\n",
    "# Initialize the XGBoostClassifier\n",
    "xgbmodel = xgboost.XGBClassifier(**bst_params, use_label_encoder=False)\n",
    "\n",
    "# Train the model, using the wandb_callback for logging\n",
    "xgbmodel.fit(X_train, y_train, eval_set=[(X_test, y_test)], \n",
    "             early_stopping_rounds=run.config['early_stopping_rounds'],\n",
    "             callbacks=[wandb_callback()])\n",
    "\n",
    "bstr = xgbmodel.get_booster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the booster to disk\n",
    "model_name = f'{run.name}_model.json'\n",
    "bstr.save_model(model_name)\n",
    "\n",
    "# Get the booster's config\n",
    "#config = json.loads(bstr.save_config())\n",
    "\n",
    "# Log the trained model to W&B Artifacts, including the booster's config\n",
    "model_art = wandb.Artifact(name=model_name, type='model',) #metadata=dict(config))\n",
    "model_art.add_file(model_name)\n",
    "run.log_artifact(model_art);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log booster metrics\n",
    "run.summary[\"best_score\"] = bstr.best_score\n",
    "run.summary[\"best_iteration\"] = bstr.best_iteration\n",
    "#run.summary[\"best_ntree_limit\"] = bstr.best_ntree_limit\n",
    "\n",
    "# Log validation metrics\n",
    "preds = xgbmodel.predict(X_test)\n",
    "run.summary[\"f1_score\"] = f1_score(y_test, preds, average='macro')\n",
    "run.summary[\"accuracy\"] = accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finish the W&B Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>iteration</td><td>▁▂▂▃▄▅▅▆▇▇▁▂▂▃▄▅▅▆▇█▁▂▂▃▄▅▆▆▇█▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>valid_0_multi_logloss</td><td>█▆▅▄▃▃▂▂▁▁█▆▅▄▃▃▂▂▁▁█▆▅▄▃▃▂▂▁▁▇▆▄▄▃▃▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.32011</td></tr><tr><td>best_iteration</td><td>25</td></tr><tr><td>f1_score</td><td>0.31565</td></tr><tr><td>iteration</td><td>24</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fearless-fog-25</strong> at: <a href='https://wandb.ai/ivanich/Course%20contest/runs/vv619c6y' target=\"_blank\">https://wandb.ai/ivanich/Course%20contest/runs/vv619c6y</a><br/>Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230424_131348-vv619c6y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ❄️ Ресурсы\n",
    "\n",
    "Вот несколько релевантных ссылок по теме:\n",
    "\n",
    "\n",
    "* Ознакомьтесь с [официальной документацией](https://docs.wandb.ai/), чтобы узнать больше о передовых методах работы и дополнительных функциях.\n",
    "\n",
    "* Ознакомьтесь с [репозиторием GitHub с примерами](https://github.com/wandb/examples), где вы найдете тщательно отобранные и минимальные примеры. Это может быть хорошей отправной точкой.\n",
    "\n",
    "Вот некоторые другие Kaggle ноутбуки с использованием Weights & Biases, которые могут быть вам полезны.\n",
    "\n",
    "* [EfficientNet+Mixup+K-Fold using TF and wandb](https://www.kaggle.com/ayuraj/efficientnet-mixup-k-fold-using-tf-and-wandb)\n",
    "\n",
    "* [HPA: Segmentation Mask Visualization with W&B](https://www.kaggle.com/ayuraj/hpa-segmentation-mask-visualization-with-w-b)\n",
    "\n",
    "* [HPA: Multi-Label Classification with TF and W&B](https://www.kaggle.com/ayuraj/hpa-multi-label-classification-with-tf-and-w-b)\n",
    "\n",
    "* [🐦BirdCLEF: Quick EDA with W&B](https://www.kaggle.com/ayuraj/birdclef-quick-eda-with-w-b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
